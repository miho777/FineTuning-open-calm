{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1nQpULH6zAYbe73Qz-bKw4Wmhqz-eBHnu",
      "authorship_tag": "ABX9TyNtYHIMojGzoVF87mlyHcx4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miho777/FineTuning-open-calm/blob/main/Piper_FineTuning_0401b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "O81jeh5KqXqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "import transformers\n",
        "import torch"
      ],
      "metadata": {
        "id": "hV9ILFpDoxVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルから追加学習データを読み込む\n",
        "df = pd.read_csv(\"/content/redfish_llm.csv\")  # ***ここは揮発領域***"
      ],
      "metadata": {
        "id": "IaC1Cqnfo1oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.shape"
      ],
      "metadata": {
        "id": "tNdT1-ha_9Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 追加学習データの表示\n",
        "df"
      ],
      "metadata": {
        "id": "lGAt6MFcqvfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ベースとなるLLMモデルの指定\n",
        "# cyberagent/open-calm-small\n",
        "# cyberagent/open-calm-medium\n",
        "# cyberagent/open-calm-large\n",
        "base_model = \"cyberagent/open-calm-medium\"\n",
        "\n",
        "# トークナイザーの読み込み\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# ベースモデルの読み込み\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "# 追加学習データ用の前処理\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"input\"], return_tensors=\"pt\")\n",
        "    # return tokenizer(examples[\"input\"], padding=\"max_length\", max_length=8, truncation=True, return_tensors=\"pt\")\n",
        "    # return tokenizer(examples[\"input\"], padding=True, max_length=24, truncation=True, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "q70iQ8b7o7KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 追加学習データの加工\n",
        "train_dataset = Dataset.from_pandas(df)\n",
        "data = DatasetDict(\n",
        "    {\n",
        "        \"train\": train_dataset,\n",
        "    }\n",
        ")\n",
        "# train_dataset = train_dataset.map(preprocess_function)\n",
        "data = data.map(lambda samples: tokenizer(samples[\"output\"]), batched=True)"
      ],
      "metadata": {
        "id": "_wm_pvGSo-wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"train\"][1]"
      ],
      "metadata": {
        "id": "nTQmr74cUNgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine Tuningの設定\n",
        "# *** pip XXXのエラーが消えない場合は、ランタイム >セッション再起動　で解決 ***\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        # per_device_train_batch_size=4,\n",
        "        # gradient_accumulation_steps=4,\n",
        "        # warmup_steps=50,\n",
        "        # # max_steps=500,\n",
        "        # # warmup_steps=5,\n",
        "        # # max_steps=200, # Epoch\n",
        "        # max_steps=10, # Epoch\n",
        "        # learning_rate=2e-4,\n",
        "        # #fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"./output\",\n",
        "        num_train_epochs = 5,\n",
        "        # per_device_train_batch_size = 4,\n",
        "        warmup_steps = 10,\n",
        "        weight_decay = 0.1,\n",
        "        save_steps = 10,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")"
      ],
      "metadata": {
        "id": "inh1XZHMXUFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ファインチューニングの実行\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "J7Oz51UOpC6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 推論 ###\n",
        "\n",
        "input_text = \"日本の有名な山は？\"\n",
        "# input_text = \"日本の有名な観光地は？\"\n",
        "# input_text = \"ネットワークコレクションのURIは何？\"\n",
        "\n",
        "# トークナイズ\n",
        "input = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# モデルによる推論実行\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**input, max_new_tokens=24, pad_token_id=tokenizer.pad_token_id,)\n",
        "\n",
        "# 出力テンソルをデコード（文字化）\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# 結果を出力\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "V-QDyqtpDwvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習後のモデルを保存\n",
        "trainer.save_model(\"./output/fine_tuned_model\")"
      ],
      "metadata": {
        "id": "0V0mQ18LpF61"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}